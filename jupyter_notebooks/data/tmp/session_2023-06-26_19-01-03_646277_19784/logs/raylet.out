[2023-06-26 19:01:06,824 I 17880 19512] (raylet.exe) io_service_pool.cc:35: IOServicePool is running with 1 io_service.
[2023-06-26 19:01:06,826 I 17880 19512] (raylet.exe) store_runner.cc:32: Allowing the Plasma store to use up to 4.8776GB of memory.
[2023-06-26 19:01:06,827 I 17880 19512] (raylet.exe) store_runner.cc:48: Starting object store with directory C:\Users\ddegtyar\AppData\Local\Temp, fallback C:/Users/ddegtyar/PycharmProjects/curriculumagent/jupyter_notebooks/data/tmp, and huge page support disabled
[2023-06-26 19:01:06,828 I 17880 19852] (raylet.exe) store.cc:554: ========== Plasma store: =================
Current usage: 0 / 4.8776 GB
- num bytes created total: 0
0 pending objects of total size 0MB
- objects spillable: 0
- bytes spillable: 0
- objects unsealed: 0
- bytes unsealed: 0
- objects in use: 0
- bytes in use: 0
- objects evictable: 0
- bytes evictable: 0

- objects created by worker: 0
- bytes created by worker: 0
- objects restored: 0
- bytes restored: 0
- objects received: 0
- bytes received: 0
- objects errored: 0
- bytes errored: 0

[2023-06-26 19:01:06,843 I 17880 19512] (raylet.exe) grpc_server.cc:140: ObjectManager server started, listening on port 53809.
[2023-06-26 19:01:06,845 I 17880 19512] (raylet.exe) worker_killing_policy.cc:101: Running GroupByOwner policy.
[2023-06-26 19:01:06,845 W 17880 19512] (raylet.exe) memory_monitor.cc:67: Not running MemoryMonitor. It is currently supported only on Linux.
[2023-06-26 19:01:06,845 I 17880 19512] (raylet.exe) node_manager.cc:295: Initializing NodeManager with ID 8e8c49aa709f00a995251f2d5e8f7fa4c0d535e6babe68df9e2b7369
[2023-06-26 19:01:06,847 I 17880 19512] (raylet.exe) grpc_server.cc:140: NodeManager server started, listening on port 53811.
[2023-06-26 19:01:06,878 I 17880 19816] (raylet.exe) agent_manager.cc:109: Monitor agent process with id 15724, register timeout 100000ms.
[2023-06-26 19:01:06,885 I 17880 19512] (raylet.exe) event.cc:234: Set ray event level to warning
[2023-06-26 19:01:06,886 I 17880 19512] (raylet.exe) event.cc:342: Ray Event initialized for RAYLET
[2023-06-26 19:01:06,887 I 17880 19512] (raylet.exe) raylet.cc:117: Raylet of id, 8e8c49aa709f00a995251f2d5e8f7fa4c0d535e6babe68df9e2b7369 started. Raylet consists of node_manager and object_manager. node_manager address: 127.0.0.1:53811 object_manager address: 127.0.0.1:53809 hostname: ieecnb203872
[2023-06-26 19:01:06,894 I 17880 19512] (raylet.exe) node_manager.cc:525: [state-dump] Event stats:
[state-dump] 
[state-dump] 
[state-dump] Global stats: 20 total (11 active)
[state-dump] Queueing time: mean = 5.376 ms, max = 40.739 ms, min = 18.000 us, total = 107.515 ms
[state-dump] Execution time:  mean = 3.467 ms, total = 69.338 ms
[state-dump] Event stats:
[state-dump] 	PeriodicalRunner.RunFnPeriodically - 9 total (2 active, 1 running), CPU time: mean = 559.644 us, total = 5.037 ms
[state-dump] 	UNKNOWN - 3 total (3 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.debug_state_dump - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeInfoGcsService.grpc_client.GetInternalConfig - 1 total (0 active), CPU time: mean = 63.589 ms, total = 63.589 ms
[state-dump] 	InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), CPU time: mean = 712.900 us, total = 712.900 us
[state-dump] 	InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	RayletWorkerPool.deadline_timer.kill_idle_workers - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.record_metrics - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.flush_free_objects - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 
[state-dump] NodeManager:
[state-dump] Node ID: 8e8c49aa709f00a995251f2d5e8f7fa4c0d535e6babe68df9e2b7369
[state-dump] Node name: 127.0.0.1
[state-dump] InitialConfigResources: {memory: 97552072710000, CPU: 30000, object_store_memory: 48776036350000, node:127.0.0.1: 10000}
[state-dump] ClusterTaskManager:
[state-dump] ========== Node: 8e8c49aa709f00a995251f2d5e8f7fa4c0d535e6babe68df9e2b7369 =================
[state-dump] Infeasible queue length: 0
[state-dump] Schedule queue length: 0
[state-dump] Dispatch queue length: 0
[state-dump] num_waiting_for_resource: 0
[state-dump] num_waiting_for_plasma_memory: 0
[state-dump] num_waiting_for_remote_node_resources: 0
[state-dump] num_worker_not_started_by_job_config_not_exist: 0
[state-dump] num_worker_not_started_by_registration_timeout: 0
[state-dump] num_tasks_waiting_for_workers: 0
[state-dump] num_cancelled_tasks: 0
[state-dump] cluster_resource_scheduler state: 
[state-dump] Local id: 4022986116431820870 Local resources: {memory: [97552072710000]/[97552072710000], node:127.0.0.1: [10000]/[10000], object_store_memory: [48776036350000]/[48776036350000], CPU: [30000]/[30000]}node id: 4022986116431820870{memory: 97552072710000/97552072710000, CPU: 30000/30000, object_store_memory: 48776036350000/48776036350000, node:127.0.0.1: 10000/10000}{ "placment group locations": [], "node to bundles": []}
[state-dump] Waiting tasks size: 0
[state-dump] Number of executing tasks: 0
[state-dump] Number of pinned task arguments: 0
[state-dump] Number of total spilled tasks: 0
[state-dump] Number of spilled waiting tasks: 0
[state-dump] Number of spilled unschedulable tasks: 0
[state-dump] Resource usage {
[state-dump] }
[state-dump] Running tasks by scheduling class:
[state-dump] ==================================================
[state-dump] 
[state-dump] ClusterResources:
[state-dump] LocalObjectManager:
[state-dump] - num pinned objects: 0
[state-dump] - pinned objects size: 0
[state-dump] - num objects pending restore: 0
[state-dump] - num objects pending spill: 0
[state-dump] - num bytes pending spill: 0
[state-dump] - num bytes currently spilled: 0
[state-dump] - cumulative spill requests: 0
[state-dump] - cumulative restore requests: 0
[state-dump] - spilled objects pending delete: 0
[state-dump] 
[state-dump] ObjectManager:
[state-dump] - num local objects: 0
[state-dump] - num unfulfilled push requests: 0
[state-dump] - num object pull requests: 0
[state-dump] - num chunks received total: 0
[state-dump] - num chunks received failed (all): 0
[state-dump] - num chunks received failed / cancelled: 0
[state-dump] - num chunks received failed / plasma error: 0
[state-dump] Event stats:
[state-dump] Global stats: 0 total (0 active)
[state-dump] Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] Execution time:  mean = -nan(ind) s, total = 0.000 s
[state-dump] Event stats:
[state-dump] PushManager:
[state-dump] - num pushes in flight: 0
[state-dump] - num chunks in flight: 0
[state-dump] - num chunks remaining: 0
[state-dump] - max chunks allowed: 409
[state-dump] OwnershipBasedObjectDirectory:
[state-dump] - num listeners: 0
[state-dump] - cumulative location updates: 0
[state-dump] - num location updates per second: 3000.000
[state-dump] - num location lookups per second: 0.000
[state-dump] - num locations added per second: 0.000
[state-dump] - num locations removed per second: 3000.000
[state-dump] BufferPool:
[state-dump] - create buffer state map size: 0
[state-dump] PullManager:
[state-dump] - num bytes available for pulled objects: 4877603635
[state-dump] - num bytes being pulled (all): 0
[state-dump] - num bytes being pulled / pinned: 0
[state-dump] - get request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - wait request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - task request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - first get request bundle: N/A
[state-dump] - first wait request bundle: N/A
[state-dump] - first task request bundle: N/A
[state-dump] - num objects queued: 0
[state-dump] - num objects actively pulled (all): 0
[state-dump] - num objects actively pulled / pinned: 0
[state-dump] - num bundles being pulled: 0
[state-dump] - num pull retries: 0
[state-dump] - max timeout seconds: 0
[state-dump] - max timeout request is already processed. No entry.
[state-dump] 
[state-dump] WorkerPool:
[state-dump] - registered jobs: 0
[state-dump] - process_failed_job_config_missing: 0
[state-dump] - process_failed_rate_limited: 0
[state-dump] - process_failed_pending_registration: 0
[state-dump] - process_failed_runtime_env_setup_failed: 0
[state-dump] - num PYTHON workers: 0
[state-dump] - num PYTHON drivers: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num idle workers: 0
[state-dump] TaskDependencyManager:
[state-dump] - task deps map size: 0
[state-dump] - get req map size: 0
[state-dump] - wait req map size: 0
[state-dump] - local objects map size: 0
[state-dump] WaitManager:
[state-dump] - num active wait requests: 0
[state-dump] Subscriber:
[state-dump] Channel WORKER_OBJECT_EVICTION
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_REF_REMOVED_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_OBJECT_LOCATIONS_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] num async plasma notifications: 0
[state-dump] Remote node managers: 
[state-dump] Event stats:
[state-dump] Global stats: 20 total (11 active)
[state-dump] Queueing time: mean = 5.376 ms, max = 40.739 ms, min = 18.000 us, total = 107.515 ms
[state-dump] Execution time:  mean = 3.467 ms, total = 69.338 ms
[state-dump] Event stats:
[state-dump] 	PeriodicalRunner.RunFnPeriodically - 9 total (2 active, 1 running), CPU time: mean = 559.644 us, total = 5.037 ms
[state-dump] 	UNKNOWN - 3 total (3 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.debug_state_dump - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeInfoGcsService.grpc_client.GetInternalConfig - 1 total (0 active), CPU time: mean = 63.589 ms, total = 63.589 ms
[state-dump] 	InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), CPU time: mean = 712.900 us, total = 712.900 us
[state-dump] 	InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	RayletWorkerPool.deadline_timer.kill_idle_workers - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.record_metrics - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.flush_free_objects - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
[state-dump] DebugString() time ms: 0
[state-dump] 
[state-dump] 
[2023-06-26 19:01:06,895 I 17880 19512] (raylet.exe) accessor.cc:611: Received notification for node id = 8e8c49aa709f00a995251f2d5e8f7fa4c0d535e6babe68df9e2b7369, IsAlive = 1
[2023-06-26 19:01:06,985 I 17880 19512] (raylet.exe) worker_pool.cc:489: Started worker process with pid 1632, the token is 0
[2023-06-26 19:01:07,011 I 17880 19512] (raylet.exe) worker_pool.cc:489: Started worker process with pid 18932, the token is 1
[2023-06-26 19:01:07,037 I 17880 19512] (raylet.exe) worker_pool.cc:489: Started worker process with pid 19264, the token is 2
[2023-06-26 19:01:09,468 I 17880 19852] (raylet.exe) object_store.cc:35: Object store current usage 8e-09 / 4.8776 GB.
[2023-06-26 19:01:09,604 I 17880 19512] (raylet.exe) node_manager.cc:607: New job has started. Job id 01000000 Driver pid 19784 is dead: 0 driver address: 127.0.0.1
[2023-06-26 19:01:09,604 I 17880 19512] (raylet.exe) worker_pool.cc:672: Job 01000000 already started in worker pool.
[2023-06-26 19:01:10,687 I 17880 19512] (raylet.exe) agent_manager.cc:40: HandleRegisterAgent, ip: 127.0.0.1, port: 57171, id: 15724
[2023-06-26 19:01:15,727 I 17880 19512] (raylet.exe) node_manager.cc:1449: NodeManager::DisconnectClient, disconnect_type=3, has creation task exception = true
[2023-06-26 19:01:15,728 I 17880 19512] (raylet.exe) node_manager.cc:1552: Driver (pid=19784) is disconnected. job_id: 01000000
[2023-06-26 19:01:15,733 I 17880 19512] (raylet.exe) node_manager.cc:607: New job has started. Job id 01000000 Driver pid 19784 is dead: 1 driver address: 127.0.0.1
[2023-06-26 19:01:15,733 I 17880 19512] (raylet.exe) worker_pool.cc:672: Job 01000000 already started in worker pool.
[2023-06-26 19:01:15,901 I 17880 19512] (raylet.exe) main.cc:303: Raylet received SIGTERM, shutting down...
[2023-06-26 19:01:15,901 I 17880 19512] (raylet.exe) accessor.cc:435: Unregistering node info, node id = 8e8c49aa709f00a995251f2d5e8f7fa4c0d535e6babe68df9e2b7369
[0m[2023-06-26 19:01:15,904 I 17880 19512] (raylet.exe) io_service_pool.cc:47: IOServicePool is stopped.
[2023-06-26 19:01:16,050 I 17880 19816] (raylet.exe) agent_manager.cc:131: Agent process with id 15724 exited, exit code 21. ip 127.0.0.1. id 15724
[2023-06-26 19:01:16,050 E 17880 19816] (raylet.exe) agent_manager.cc:135: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. Agent can fail when
- The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.
- The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/dashboard_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
- The agent is killed by the OS (e.g., out of memory).
[2023-06-26 19:01:16,060 E 17880 19816] (raylet.exe) logging.cc:361: *** SIGTERM received at time=1687798876 ***
[2023-06-26 19:01:16,060 E 17880 19816] (raylet.exe) logging.cc:361:     @   00007FF6D25C0F6C  (unknown)  (unknown)
[2023-06-26 19:01:16,061 E 17880 19816] (raylet.exe) logging.cc:361:     @   00007FF6D25C03DE  (unknown)  (unknown)
[2023-06-26 19:01:16,061 E 17880 19816] (raylet.exe) logging.cc:361:     @   00007FFB2C711BB2  (unknown)  configthreadlocale
[2023-06-26 19:01:16,061 E 17880 19816] (raylet.exe) logging.cc:361:     @   00007FFB2EDE7604  (unknown)  BaseThreadInitThunk
[2023-06-26 19:01:16,062 E 17880 19816] (raylet.exe) logging.cc:361:     @   00007FFB2F0C26A1  (unknown)  RtlUserThreadStart
